{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e4d353",
   "metadata": {},
   "source": [
    "## Image Classification using Logistic Regression in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5f586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.6.15)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d739c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d232b5",
   "metadata": {},
   "source": [
    "## Exploring The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f41dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torchvision\n",
    "#from torchvision import MNI\n",
    "import  torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757aade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  C:\\Users\\hp\\anaconda3\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  C:\\Users\\hp\\anaconda3\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  C:\\Users\\hp\\anaconda3\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  C:\\Users\\hp\\anaconda3\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  C:\\Users\\hp\\anaconda3\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "-f option requires 1 argument\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea16b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a788e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: torch==1.12.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0b50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data \n",
    "dataset = datasets.MNIST(root = 'data/', download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c70cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f840fbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x23BDC8DBB50>, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root = 'data/', train = False)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ead48e",
   "metadata": {},
   "source": [
    "## Plotting Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702a7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfefbf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img , lbl = dataset[0]\n",
    "plt.imshow(img)\n",
    "print('Label:', lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d2148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img , lbl = dataset[10]\n",
    "plt.imshow(img,cmap='gray')\n",
    "print('Label:', lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5569f",
   "metadata": {},
   "source": [
    "### Pytorch doesnot know how to work with  images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9fb9c",
   "metadata": {},
   "source": [
    "We need to convert the images into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246318bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ea5b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNSIT DATASET(images and labels)\n",
    "dataset = datasets.MNIST(root ='data/', train = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a7b48f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor , lbl = dataset[0]\n",
    "print(img_tensor.shape, lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1ed7f",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3237943",
   "metadata": {},
   "source": [
    "1. Training set : Used to train the model\n",
    "2. Training set : Used to Evaluate the model while training\n",
    "3. Test set : Used to compare Different model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "222a5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_ind(n,val):\n",
    "    # set size of validation set\n",
    "    n_val = int(val * n)\n",
    "    # create random permutation of 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27078b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind , val_ind = split_ind(len(dataset), val = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd88d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ind), len(val_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abb75750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [27471  9654 57021 59460 36025 25478 35720 43324 56012  8637 31515 10851\n",
      " 45072 40310 24697 29787  6985 44235  8640 10391]\n"
     ]
    }
   ],
   "source": [
    "print('sample:',val_ind[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e543da",
   "metadata": {},
   "source": [
    "**We now create  pytorch data loaders for each of these using a \"subsetrandomsample\", which samples elements randomly from a given list of indices while greating batches of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02b72906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "801c82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "# training sampler and data loader\n",
    "train_sampler = SubsetRandomSampler(train_ind)\n",
    "train_loader = DataLoader(dataset, batch_size, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f97f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation sampler and data loader\n",
    "val_samp = SubsetRandomSampler(val_ind)\n",
    "val_loader = DataLoader(dataset, batch_size, sampler=val_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012bb724",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1d3a3",
   "metadata": {},
   "source": [
    "- We can use nn.Linear to  create the model instead of defining and initilizing the matrices manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cb6d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28* 28\n",
    "num_classes = 10\n",
    "# logistic regression model \n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af4341eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0288, -0.0216,  0.0290,  ...,  0.0147, -0.0134,  0.0133],\n",
       "        [ 0.0063,  0.0325,  0.0061,  ...,  0.0224,  0.0348, -0.0145],\n",
       "        [-0.0234, -0.0170, -0.0244,  ...,  0.0193, -0.0138,  0.0194],\n",
       "        ...,\n",
       "        [-0.0226,  0.0329, -0.0213,  ..., -0.0022, -0.0184, -0.0293],\n",
       "        [-0.0171,  0.0316, -0.0339,  ...,  0.0295, -0.0203, -0.0040],\n",
       "        [-0.0109,  0.0200,  0.0289,  ..., -0.0222,  0.0066,  0.0192]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases\n",
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bde80788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0164,  0.0014,  0.0018, -0.0233, -0.0007,  0.0174,  0.0278, -0.0063,\n",
       "        -0.0275,  0.0040], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4485cca",
   "metadata": {},
   "source": [
    "**We take first batch of 100 images from our dataset, and pass them into our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ef74e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 1, 6, 7, 3, 3, 6, 8, 8, 8, 1, 1, 1, 8, 9, 8, 8, 4, 5, 7, 5, 7, 2,\n",
      "        2, 9, 7, 2, 8, 1, 7, 2, 9, 0, 3, 1, 7, 4, 7, 2, 6, 2, 6, 1, 3, 3, 1, 4,\n",
      "        2, 7, 6, 8, 2, 3, 3, 2, 9, 2, 2, 1, 7, 2, 7, 3, 1, 5, 1, 5, 8, 2, 9, 9,\n",
      "        6, 2, 7, 3, 2, 1, 5, 3, 8, 4, 4, 4, 1, 4, 6, 4, 4, 8, 0, 0, 7, 4, 5, 4,\n",
      "        7, 7, 0, 0])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2800x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(lbl)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2800x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for img, lbl in train_loader:\n",
    "    print(lbl)\n",
    "    print(img.shape)\n",
    "    outputs = model(img)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcef768",
   "metadata": {},
   "source": [
    "- It seems that our training shape is not same so lets solve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71c4f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1,784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c318914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0159,  0.0231,  0.0334,  ..., -0.0335,  0.0332, -0.0329],\n",
       "         [-0.0132, -0.0281, -0.0149,  ..., -0.0209, -0.0156,  0.0313],\n",
       "         [ 0.0111, -0.0353,  0.0191,  ...,  0.0184,  0.0065, -0.0058],\n",
       "         ...,\n",
       "         [-0.0027,  0.0174,  0.0071,  ...,  0.0048,  0.0213, -0.0241],\n",
       "         [ 0.0232,  0.0180, -0.0091,  ..., -0.0225,  0.0047, -0.0355],\n",
       "         [-0.0228, -0.0230, -0.0274,  ...,  0.0088, -0.0251, -0.0125]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.1979e-02, -3.3908e-02, -6.4503e-05,  3.0590e-02, -2.0186e-02,\n",
       "         -2.4779e-02, -2.9092e-02,  3.5431e-02, -2.2836e-02,  1.6756e-02],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38130070",
   "metadata": {},
   "source": [
    "**Our new custom model can be used in the exact same way before . Lets see if it works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa2c5061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape:  torch.Size([100, 10])\n",
      "sample outputs:\n",
      "  tensor([[-0.2191, -0.1782, -0.1468,  0.1968,  0.2130, -0.1443, -0.1483,  0.2601,\n",
      "         -0.0407,  0.1600],\n",
      "        [-0.0220,  0.2116, -0.0450,  0.6280, -0.0863, -0.0091, -0.1462, -0.0692,\n",
      "          0.4427,  0.0117]])\n"
     ]
    }
   ],
   "source": [
    "for img , lbl in train_loader:\n",
    "    outputs = model(img)\n",
    "    break\n",
    "print('outputs.shape: ', outputs.shape)\n",
    "print('sample outputs:\\n ', outputs[:2].data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd27df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b52cb",
   "metadata": {},
   "source": [
    "**The softmax funcrtion is included in the torch.nn.functional package, and requires us to specify a dimension along which the softmax must be applied**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d159a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample prob: \n",
      " tensor([[0.0794, 0.0827, 0.0853, 0.1203, 0.1223, 0.0856, 0.0852, 0.1282, 0.0949,\n",
      "         0.1160],\n",
      "        [0.0864, 0.1092, 0.0845, 0.1656, 0.0810, 0.0876, 0.0763, 0.0824, 0.1376,\n",
      "         0.0894]])\n",
      "Sum:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax for each output row \n",
    "probs = F.softmax(outputs, dim =1)\n",
    "# sample\n",
    "print(\"sample prob: \\n\", probs[:2].data)\n",
    "# Add up probability\n",
    "print('Sum: ', torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52c17eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 3, 3, 3, 9, 3, 2, 3, 3, 3, 1, 3, 3, 3, 4, 3, 3, 3, 1, 3, 3, 3, 7, 3,\n",
      "        3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 9,\n",
      "        3, 7, 3, 3, 3, 1, 3, 7, 9, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 8, 3, 3, 3, 3,\n",
      "        3, 3, 7, 7, 3, 3, 3, 3, 8, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 8, 3, 3, 4, 3,\n",
      "        1, 7, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs,dim = 1)\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2cf1818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 3, 4, 8, 9, 4, 6, 6, 5, 3, 2, 7, 7, 2, 0, 3, 2, 1, 5, 0, 2, 8, 8,\n",
       "        6, 0, 6, 1, 6, 8, 2, 2, 4, 6, 9, 8, 8, 5, 0, 8, 4, 2, 8, 7, 7, 1, 9, 7,\n",
       "        9, 1, 9, 4, 9, 3, 4, 2, 3, 4, 5, 3, 9, 7, 6, 0, 2, 7, 9, 7, 2, 2, 7, 6,\n",
       "        2, 4, 1, 8, 7, 0, 6, 3, 7, 7, 6, 6, 5, 2, 1, 9, 2, 9, 7, 1, 6, 7, 3, 5,\n",
       "        1, 8, 8, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b2fc9",
   "metadata": {},
   "source": [
    "## Evaluation Metric and loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e134a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1,l2):\n",
    "    return torch.sum(l1 == l2).item()/len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dc63515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24c139",
   "metadata": {},
   "source": [
    "Unlike accuracy , cross-entropy is a continuous and differentiable function that also provides good feedback for incremental improvements in the model .This makes a good choice for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95385021",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a293ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3616, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for current batch \n",
    "loss = loss_fun(outputs, lbl)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1fe1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "opt = torch.optim.SGD(model.parameters(),lr= learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62b77e",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d8058",
   "metadata": {},
   "source": [
    "- Now we have to define data loaders, moel , loss func and optimizer we are ready to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8377d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt= None, metric = None):\n",
    "    #calculate loss\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        # compute gradients \n",
    "        loss.backward()\n",
    "        # update \n",
    "        opt.step()\n",
    "        # reset\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    metric_res = None\n",
    "    if metric is not None:\n",
    "        # conpute metric\n",
    "        metric_res = metric(preds,yb)\n",
    "    return loss.item(), len(xb), metric_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0120cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fun,valid_dl, metric = None):\n",
    "    with torch.no_grad():\n",
    "        # pass eacg batch through model\n",
    "        res = [loss_batch(model, loss_fun,xb,yb, metric = metric)\n",
    "              for xb,yb in valid_dl]\n",
    "        # separate \n",
    "        losses, nums,metrices = zip(*res)\n",
    "        # Total size \n",
    "        total = np.sum(nums)\n",
    "        # avg loss\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrices, nums)) / total \n",
    "        return avg_loss ,total , avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ffacaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, lbl):\n",
    "    _, preds = torch.max(outputs, dim = 1)\n",
    "    return torch.sum(preds == lbl).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbfd44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2.3283, Acc :0.0841\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, loss_fun, val_loader, metric = accuracy)\n",
    "print('Loss:{:.4f}, Acc :{:.4f}'.format(val_loss,val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407f2e1",
   "metadata": {},
   "source": [
    "**We can now define fit model  function quite easily using loss_batch and evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b2acf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs , model , loss_fn, opt, train_dl, valid_dl, metric = None):\n",
    "    for epoch in range(epochs):\n",
    "        #Training\n",
    "        for xb,yb in train_dl:\n",
    "            loss, _,_ = loss_batch(model, loss_fn, xb, yb ,opt)\n",
    "            # Evaluation \n",
    "            res = evaluate( model , loss_fn,  valid_dl, metric)\n",
    "            val_loss , total, val_metric = res\n",
    "            \n",
    "            # progress\n",
    "            if metric is not None:\n",
    "                print('Epoch [{}/{}], loss:{:.4f}'.format(epoch+1, epochs, val_loss))\n",
    "                \n",
    "            else :\n",
    "                print('Epoch [{}/{}], loss:{:.4f}, {}: {:.4f}'.format(epoch+1, epochs, val_loss), metric.__name__, val_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df9edd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine model \n",
    "model = MnistModel()\n",
    "opt = torch.optim.SGD(model.parameters(),lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec79588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], loss:2.2750\n",
      "Epoch [1/5], loss:2.2739\n",
      "Epoch [1/5], loss:2.2728\n",
      "Epoch [1/5], loss:2.2718\n",
      "Epoch [1/5], loss:2.2707\n",
      "Epoch [1/5], loss:2.2696\n",
      "Epoch [1/5], loss:2.2686\n",
      "Epoch [1/5], loss:2.2676\n",
      "Epoch [1/5], loss:2.2666\n",
      "Epoch [1/5], loss:2.2656\n",
      "Epoch [1/5], loss:2.2646\n",
      "Epoch [1/5], loss:2.2636\n",
      "Epoch [1/5], loss:2.2625\n",
      "Epoch [1/5], loss:2.2613\n",
      "Epoch [1/5], loss:2.2601\n",
      "Epoch [1/5], loss:2.2591\n",
      "Epoch [1/5], loss:2.2579\n",
      "Epoch [1/5], loss:2.2571\n",
      "Epoch [1/5], loss:2.2561\n",
      "Epoch [1/5], loss:2.2550\n",
      "Epoch [1/5], loss:2.2539\n",
      "Epoch [1/5], loss:2.2528\n",
      "Epoch [1/5], loss:2.2517\n",
      "Epoch [1/5], loss:2.2507\n",
      "Epoch [1/5], loss:2.2497\n",
      "Epoch [1/5], loss:2.2486\n",
      "Epoch [1/5], loss:2.2476\n",
      "Epoch [1/5], loss:2.2464\n",
      "Epoch [1/5], loss:2.2455\n",
      "Epoch [1/5], loss:2.2445\n",
      "Epoch [1/5], loss:2.2434\n",
      "Epoch [1/5], loss:2.2424\n",
      "Epoch [1/5], loss:2.2413\n",
      "Epoch [1/5], loss:2.2404\n",
      "Epoch [1/5], loss:2.2394\n",
      "Epoch [1/5], loss:2.2384\n",
      "Epoch [1/5], loss:2.2374\n",
      "Epoch [1/5], loss:2.2364\n",
      "Epoch [1/5], loss:2.2352\n",
      "Epoch [1/5], loss:2.2342\n",
      "Epoch [1/5], loss:2.2333\n",
      "Epoch [1/5], loss:2.2324\n",
      "Epoch [1/5], loss:2.2312\n",
      "Epoch [1/5], loss:2.2301\n",
      "Epoch [1/5], loss:2.2291\n",
      "Epoch [1/5], loss:2.2281\n",
      "Epoch [1/5], loss:2.2272\n",
      "Epoch [1/5], loss:2.2261\n",
      "Epoch [1/5], loss:2.2251\n",
      "Epoch [1/5], loss:2.2240\n",
      "Epoch [1/5], loss:2.2230\n",
      "Epoch [1/5], loss:2.2221\n",
      "Epoch [1/5], loss:2.2213\n",
      "Epoch [1/5], loss:2.2203\n",
      "Epoch [1/5], loss:2.2193\n",
      "Epoch [1/5], loss:2.2184\n",
      "Epoch [1/5], loss:2.2173\n",
      "Epoch [1/5], loss:2.2163\n",
      "Epoch [1/5], loss:2.2153\n",
      "Epoch [1/5], loss:2.2145\n",
      "Epoch [1/5], loss:2.2133\n",
      "Epoch [1/5], loss:2.2124\n",
      "Epoch [1/5], loss:2.2115\n",
      "Epoch [1/5], loss:2.2105\n",
      "Epoch [1/5], loss:2.2095\n",
      "Epoch [1/5], loss:2.2084\n",
      "Epoch [1/5], loss:2.2074\n",
      "Epoch [1/5], loss:2.2062\n",
      "Epoch [1/5], loss:2.2054\n",
      "Epoch [1/5], loss:2.2043\n",
      "Epoch [1/5], loss:2.2033\n",
      "Epoch [1/5], loss:2.2023\n",
      "Epoch [1/5], loss:2.2014\n",
      "Epoch [1/5], loss:2.2005\n",
      "Epoch [1/5], loss:2.1996\n",
      "Epoch [1/5], loss:2.1987\n",
      "Epoch [1/5], loss:2.1977\n",
      "Epoch [1/5], loss:2.1969\n",
      "Epoch [1/5], loss:2.1959\n",
      "Epoch [1/5], loss:2.1949\n",
      "Epoch [1/5], loss:2.1939\n",
      "Epoch [1/5], loss:2.1929\n",
      "Epoch [1/5], loss:2.1919\n",
      "Epoch [1/5], loss:2.1908\n",
      "Epoch [1/5], loss:2.1898\n",
      "Epoch [1/5], loss:2.1889\n",
      "Epoch [1/5], loss:2.1879\n",
      "Epoch [1/5], loss:2.1870\n",
      "Epoch [1/5], loss:2.1859\n",
      "Epoch [1/5], loss:2.1849\n",
      "Epoch [1/5], loss:2.1840\n",
      "Epoch [1/5], loss:2.1829\n",
      "Epoch [1/5], loss:2.1819\n",
      "Epoch [1/5], loss:2.1809\n",
      "Epoch [1/5], loss:2.1798\n",
      "Epoch [1/5], loss:2.1787\n",
      "Epoch [1/5], loss:2.1778\n",
      "Epoch [1/5], loss:2.1768\n",
      "Epoch [1/5], loss:2.1758\n",
      "Epoch [1/5], loss:2.1750\n",
      "Epoch [1/5], loss:2.1740\n",
      "Epoch [1/5], loss:2.1730\n",
      "Epoch [1/5], loss:2.1721\n",
      "Epoch [1/5], loss:2.1712\n",
      "Epoch [1/5], loss:2.1702\n",
      "Epoch [1/5], loss:2.1692\n",
      "Epoch [1/5], loss:2.1682\n",
      "Epoch [1/5], loss:2.1673\n",
      "Epoch [1/5], loss:2.1664\n",
      "Epoch [1/5], loss:2.1655\n",
      "Epoch [1/5], loss:2.1645\n",
      "Epoch [1/5], loss:2.1636\n",
      "Epoch [1/5], loss:2.1627\n",
      "Epoch [1/5], loss:2.1617\n",
      "Epoch [1/5], loss:2.1608\n",
      "Epoch [1/5], loss:2.1597\n",
      "Epoch [1/5], loss:2.1587\n",
      "Epoch [1/5], loss:2.1578\n",
      "Epoch [1/5], loss:2.1568\n",
      "Epoch [1/5], loss:2.1559\n",
      "Epoch [1/5], loss:2.1549\n",
      "Epoch [1/5], loss:2.1540\n",
      "Epoch [1/5], loss:2.1531\n",
      "Epoch [1/5], loss:2.1520\n",
      "Epoch [1/5], loss:2.1510\n",
      "Epoch [1/5], loss:2.1501\n",
      "Epoch [1/5], loss:2.1492\n",
      "Epoch [1/5], loss:2.1484\n",
      "Epoch [1/5], loss:2.1474\n",
      "Epoch [1/5], loss:2.1465\n",
      "Epoch [1/5], loss:2.1456\n",
      "Epoch [1/5], loss:2.1446\n",
      "Epoch [1/5], loss:2.1436\n",
      "Epoch [1/5], loss:2.1427\n",
      "Epoch [1/5], loss:2.1416\n",
      "Epoch [1/5], loss:2.1406\n",
      "Epoch [1/5], loss:2.1396\n",
      "Epoch [1/5], loss:2.1385\n",
      "Epoch [1/5], loss:2.1376\n",
      "Epoch [1/5], loss:2.1366\n",
      "Epoch [1/5], loss:2.1357\n",
      "Epoch [1/5], loss:2.1348\n",
      "Epoch [1/5], loss:2.1339\n",
      "Epoch [1/5], loss:2.1329\n",
      "Epoch [1/5], loss:2.1319\n",
      "Epoch [1/5], loss:2.1310\n",
      "Epoch [1/5], loss:2.1302\n",
      "Epoch [1/5], loss:2.1292\n",
      "Epoch [1/5], loss:2.1282\n",
      "Epoch [1/5], loss:2.1273\n",
      "Epoch [1/5], loss:2.1263\n",
      "Epoch [1/5], loss:2.1254\n",
      "Epoch [1/5], loss:2.1245\n",
      "Epoch [1/5], loss:2.1236\n",
      "Epoch [1/5], loss:2.1228\n",
      "Epoch [1/5], loss:2.1218\n",
      "Epoch [1/5], loss:2.1209\n",
      "Epoch [1/5], loss:2.1199\n",
      "Epoch [1/5], loss:2.1190\n",
      "Epoch [1/5], loss:2.1181\n",
      "Epoch [1/5], loss:2.1170\n",
      "Epoch [1/5], loss:2.1162\n",
      "Epoch [1/5], loss:2.1152\n",
      "Epoch [1/5], loss:2.1144\n",
      "Epoch [1/5], loss:2.1135\n",
      "Epoch [1/5], loss:2.1127\n",
      "Epoch [1/5], loss:2.1118\n",
      "Epoch [1/5], loss:2.1108\n",
      "Epoch [1/5], loss:2.1099\n",
      "Epoch [1/5], loss:2.1090\n",
      "Epoch [1/5], loss:2.1080\n",
      "Epoch [1/5], loss:2.1072\n",
      "Epoch [1/5], loss:2.1061\n",
      "Epoch [1/5], loss:2.1052\n",
      "Epoch [1/5], loss:2.1043\n",
      "Epoch [1/5], loss:2.1034\n",
      "Epoch [1/5], loss:2.1024\n",
      "Epoch [1/5], loss:2.1016\n",
      "Epoch [1/5], loss:2.1006\n",
      "Epoch [1/5], loss:2.0998\n",
      "Epoch [1/5], loss:2.0989\n",
      "Epoch [1/5], loss:2.0979\n",
      "Epoch [1/5], loss:2.0970\n",
      "Epoch [1/5], loss:2.0961\n",
      "Epoch [1/5], loss:2.0952\n",
      "Epoch [1/5], loss:2.0943\n",
      "Epoch [1/5], loss:2.0934\n",
      "Epoch [1/5], loss:2.0924\n",
      "Epoch [1/5], loss:2.0916\n",
      "Epoch [1/5], loss:2.0907\n",
      "Epoch [1/5], loss:2.0898\n",
      "Epoch [1/5], loss:2.0889\n",
      "Epoch [1/5], loss:2.0880\n",
      "Epoch [1/5], loss:2.0872\n",
      "Epoch [1/5], loss:2.0863\n",
      "Epoch [1/5], loss:2.0853\n",
      "Epoch [1/5], loss:2.0845\n",
      "Epoch [1/5], loss:2.0835\n",
      "Epoch [1/5], loss:2.0827\n",
      "Epoch [1/5], loss:2.0817\n",
      "Epoch [1/5], loss:2.0807\n",
      "Epoch [1/5], loss:2.0799\n",
      "Epoch [1/5], loss:2.0791\n",
      "Epoch [1/5], loss:2.0781\n",
      "Epoch [1/5], loss:2.0773\n",
      "Epoch [1/5], loss:2.0764\n",
      "Epoch [1/5], loss:2.0754\n",
      "Epoch [1/5], loss:2.0745\n",
      "Epoch [1/5], loss:2.0735\n",
      "Epoch [1/5], loss:2.0727\n",
      "Epoch [1/5], loss:2.0718\n",
      "Epoch [1/5], loss:2.0709\n",
      "Epoch [1/5], loss:2.0699\n",
      "Epoch [1/5], loss:2.0690\n",
      "Epoch [1/5], loss:2.0682\n",
      "Epoch [1/5], loss:2.0672\n",
      "Epoch [1/5], loss:2.0663\n",
      "Epoch [1/5], loss:2.0654\n",
      "Epoch [1/5], loss:2.0645\n",
      "Epoch [1/5], loss:2.0636\n",
      "Epoch [1/5], loss:2.0628\n",
      "Epoch [1/5], loss:2.0620\n",
      "Epoch [1/5], loss:2.0611\n",
      "Epoch [1/5], loss:2.0603\n",
      "Epoch [1/5], loss:2.0593\n",
      "Epoch [1/5], loss:2.0584\n",
      "Epoch [1/5], loss:2.0575\n",
      "Epoch [1/5], loss:2.0567\n",
      "Epoch [1/5], loss:2.0557\n",
      "Epoch [1/5], loss:2.0547\n",
      "Epoch [1/5], loss:2.0539\n",
      "Epoch [1/5], loss:2.0530\n",
      "Epoch [1/5], loss:2.0521\n",
      "Epoch [1/5], loss:2.0512\n",
      "Epoch [1/5], loss:2.0503\n",
      "Epoch [1/5], loss:2.0493\n",
      "Epoch [1/5], loss:2.0485\n",
      "Epoch [1/5], loss:2.0476\n",
      "Epoch [1/5], loss:2.0467\n",
      "Epoch [1/5], loss:2.0459\n",
      "Epoch [1/5], loss:2.0450\n",
      "Epoch [1/5], loss:2.0442\n",
      "Epoch [1/5], loss:2.0432\n",
      "Epoch [1/5], loss:2.0424\n",
      "Epoch [1/5], loss:2.0415\n",
      "Epoch [1/5], loss:2.0406\n",
      "Epoch [1/5], loss:2.0397\n",
      "Epoch [1/5], loss:2.0388\n",
      "Epoch [1/5], loss:2.0379\n",
      "Epoch [1/5], loss:2.0371\n",
      "Epoch [1/5], loss:2.0361\n",
      "Epoch [1/5], loss:2.0353\n",
      "Epoch [1/5], loss:2.0344\n",
      "Epoch [1/5], loss:2.0336\n",
      "Epoch [1/5], loss:2.0327\n",
      "Epoch [1/5], loss:2.0318\n",
      "Epoch [1/5], loss:2.0309\n",
      "Epoch [1/5], loss:2.0300\n",
      "Epoch [1/5], loss:2.0291\n",
      "Epoch [1/5], loss:2.0282\n",
      "Epoch [1/5], loss:2.0273\n",
      "Epoch [1/5], loss:2.0265\n",
      "Epoch [1/5], loss:2.0256\n",
      "Epoch [1/5], loss:2.0248\n",
      "Epoch [1/5], loss:2.0239\n",
      "Epoch [1/5], loss:2.0230\n",
      "Epoch [1/5], loss:2.0221\n",
      "Epoch [1/5], loss:2.0213\n",
      "Epoch [1/5], loss:2.0205\n",
      "Epoch [1/5], loss:2.0197\n",
      "Epoch [1/5], loss:2.0188\n",
      "Epoch [1/5], loss:2.0180\n",
      "Epoch [1/5], loss:2.0172\n",
      "Epoch [1/5], loss:2.0162\n",
      "Epoch [1/5], loss:2.0153\n",
      "Epoch [1/5], loss:2.0145\n",
      "Epoch [1/5], loss:2.0136\n",
      "Epoch [1/5], loss:2.0128\n",
      "Epoch [1/5], loss:2.0120\n",
      "Epoch [1/5], loss:2.0112\n",
      "Epoch [1/5], loss:2.0104\n",
      "Epoch [1/5], loss:2.0094\n",
      "Epoch [1/5], loss:2.0086\n",
      "Epoch [1/5], loss:2.0077\n",
      "Epoch [1/5], loss:2.0069\n",
      "Epoch [1/5], loss:2.0060\n",
      "Epoch [1/5], loss:2.0050\n",
      "Epoch [1/5], loss:2.0043\n",
      "Epoch [1/5], loss:2.0035\n",
      "Epoch [1/5], loss:2.0027\n",
      "Epoch [1/5], loss:2.0020\n",
      "Epoch [1/5], loss:2.0011\n",
      "Epoch [1/5], loss:2.0003\n",
      "Epoch [1/5], loss:1.9995\n",
      "Epoch [1/5], loss:1.9986\n",
      "Epoch [1/5], loss:1.9978\n",
      "Epoch [1/5], loss:1.9970\n",
      "Epoch [1/5], loss:1.9962\n",
      "Epoch [1/5], loss:1.9954\n",
      "Epoch [1/5], loss:1.9945\n",
      "Epoch [1/5], loss:1.9936\n",
      "Epoch [1/5], loss:1.9928\n",
      "Epoch [1/5], loss:1.9920\n",
      "Epoch [1/5], loss:1.9911\n",
      "Epoch [1/5], loss:1.9902\n",
      "Epoch [1/5], loss:1.9893\n",
      "Epoch [1/5], loss:1.9885\n",
      "Epoch [1/5], loss:1.9877\n",
      "Epoch [1/5], loss:1.9869\n",
      "Epoch [1/5], loss:1.9861\n",
      "Epoch [1/5], loss:1.9852\n",
      "Epoch [1/5], loss:1.9844\n",
      "Epoch [1/5], loss:1.9836\n",
      "Epoch [1/5], loss:1.9828\n",
      "Epoch [1/5], loss:1.9821\n",
      "Epoch [1/5], loss:1.9813\n",
      "Epoch [1/5], loss:1.9804\n",
      "Epoch [1/5], loss:1.9795\n",
      "Epoch [1/5], loss:1.9787\n",
      "Epoch [1/5], loss:1.9779\n",
      "Epoch [1/5], loss:1.9771\n",
      "Epoch [1/5], loss:1.9763\n",
      "Epoch [1/5], loss:1.9755\n",
      "Epoch [1/5], loss:1.9747\n",
      "Epoch [1/5], loss:1.9739\n",
      "Epoch [1/5], loss:1.9731\n",
      "Epoch [1/5], loss:1.9723\n",
      "Epoch [1/5], loss:1.9714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], loss:1.9706\n",
      "Epoch [1/5], loss:1.9698\n",
      "Epoch [1/5], loss:1.9690\n",
      "Epoch [1/5], loss:1.9682\n",
      "Epoch [1/5], loss:1.9674\n",
      "Epoch [1/5], loss:1.9667\n",
      "Epoch [1/5], loss:1.9659\n",
      "Epoch [1/5], loss:1.9650\n",
      "Epoch [1/5], loss:1.9641\n",
      "Epoch [1/5], loss:1.9632\n",
      "Epoch [1/5], loss:1.9624\n",
      "Epoch [1/5], loss:1.9616\n",
      "Epoch [1/5], loss:1.9608\n",
      "Epoch [1/5], loss:1.9600\n",
      "Epoch [1/5], loss:1.9592\n",
      "Epoch [1/5], loss:1.9583\n",
      "Epoch [1/5], loss:1.9575\n",
      "Epoch [1/5], loss:1.9567\n",
      "Epoch [1/5], loss:1.9559\n",
      "Epoch [1/5], loss:1.9551\n",
      "Epoch [1/5], loss:1.9542\n",
      "Epoch [1/5], loss:1.9534\n",
      "Epoch [1/5], loss:1.9527\n",
      "Epoch [1/5], loss:1.9519\n",
      "Epoch [1/5], loss:1.9511\n",
      "Epoch [1/5], loss:1.9503\n",
      "Epoch [1/5], loss:1.9495\n",
      "Epoch [1/5], loss:1.9488\n",
      "Epoch [1/5], loss:1.9479\n",
      "Epoch [1/5], loss:1.9471\n",
      "Epoch [1/5], loss:1.9464\n",
      "Epoch [1/5], loss:1.9456\n",
      "Epoch [1/5], loss:1.9448\n",
      "Epoch [1/5], loss:1.9441\n",
      "Epoch [1/5], loss:1.9433\n",
      "Epoch [1/5], loss:1.9425\n",
      "Epoch [1/5], loss:1.9417\n",
      "Epoch [1/5], loss:1.9409\n",
      "Epoch [1/5], loss:1.9401\n",
      "Epoch [1/5], loss:1.9393\n",
      "Epoch [1/5], loss:1.9385\n",
      "Epoch [1/5], loss:1.9377\n",
      "Epoch [1/5], loss:1.9370\n",
      "Epoch [1/5], loss:1.9361\n",
      "Epoch [1/5], loss:1.9354\n",
      "Epoch [1/5], loss:1.9347\n",
      "Epoch [1/5], loss:1.9339\n",
      "Epoch [1/5], loss:1.9331\n",
      "Epoch [1/5], loss:1.9323\n",
      "Epoch [1/5], loss:1.9315\n",
      "Epoch [1/5], loss:1.9307\n",
      "Epoch [1/5], loss:1.9298\n",
      "Epoch [1/5], loss:1.9291\n",
      "Epoch [1/5], loss:1.9283\n",
      "Epoch [1/5], loss:1.9275\n",
      "Epoch [1/5], loss:1.9268\n",
      "Epoch [1/5], loss:1.9260\n",
      "Epoch [1/5], loss:1.9252\n",
      "Epoch [1/5], loss:1.9245\n",
      "Epoch [1/5], loss:1.9237\n",
      "Epoch [1/5], loss:1.9229\n",
      "Epoch [1/5], loss:1.9221\n",
      "Epoch [1/5], loss:1.9213\n",
      "Epoch [1/5], loss:1.9205\n",
      "Epoch [1/5], loss:1.9197\n",
      "Epoch [1/5], loss:1.9190\n",
      "Epoch [1/5], loss:1.9182\n",
      "Epoch [1/5], loss:1.9174\n",
      "Epoch [1/5], loss:1.9167\n",
      "Epoch [1/5], loss:1.9159\n",
      "Epoch [1/5], loss:1.9151\n",
      "Epoch [1/5], loss:1.9144\n",
      "Epoch [1/5], loss:1.9135\n",
      "Epoch [1/5], loss:1.9128\n",
      "Epoch [1/5], loss:1.9121\n",
      "Epoch [1/5], loss:1.9113\n",
      "Epoch [1/5], loss:1.9105\n",
      "Epoch [1/5], loss:1.9097\n",
      "Epoch [1/5], loss:1.9088\n",
      "Epoch [1/5], loss:1.9081\n",
      "Epoch [1/5], loss:1.9072\n",
      "Epoch [1/5], loss:1.9065\n",
      "Epoch [1/5], loss:1.9057\n",
      "Epoch [1/5], loss:1.9049\n",
      "Epoch [1/5], loss:1.9042\n",
      "Epoch [1/5], loss:1.9034\n",
      "Epoch [1/5], loss:1.9026\n",
      "Epoch [1/5], loss:1.9018\n",
      "Epoch [1/5], loss:1.9010\n",
      "Epoch [1/5], loss:1.9003\n",
      "Epoch [1/5], loss:1.8995\n",
      "Epoch [1/5], loss:1.8987\n",
      "Epoch [1/5], loss:1.8979\n",
      "Epoch [1/5], loss:1.8972\n",
      "Epoch [1/5], loss:1.8964\n",
      "Epoch [1/5], loss:1.8956\n",
      "Epoch [1/5], loss:1.8948\n",
      "Epoch [1/5], loss:1.8941\n",
      "Epoch [1/5], loss:1.8933\n",
      "Epoch [1/5], loss:1.8926\n",
      "Epoch [1/5], loss:1.8918\n",
      "Epoch [1/5], loss:1.8910\n",
      "Epoch [1/5], loss:1.8903\n",
      "Epoch [1/5], loss:1.8895\n",
      "Epoch [1/5], loss:1.8888\n",
      "Epoch [1/5], loss:1.8881\n",
      "Epoch [1/5], loss:1.8874\n",
      "Epoch [1/5], loss:1.8867\n",
      "Epoch [1/5], loss:1.8859\n",
      "Epoch [1/5], loss:1.8852\n",
      "Epoch [1/5], loss:1.8844\n",
      "Epoch [1/5], loss:1.8837\n",
      "Epoch [1/5], loss:1.8830\n",
      "Epoch [1/5], loss:1.8822\n",
      "Epoch [1/5], loss:1.8815\n",
      "Epoch [1/5], loss:1.8807\n",
      "Epoch [1/5], loss:1.8800\n",
      "Epoch [1/5], loss:1.8792\n",
      "Epoch [1/5], loss:1.8785\n",
      "Epoch [1/5], loss:1.8777\n",
      "Epoch [1/5], loss:1.8769\n",
      "Epoch [1/5], loss:1.8761\n",
      "Epoch [1/5], loss:1.8754\n",
      "Epoch [1/5], loss:1.8747\n",
      "Epoch [1/5], loss:1.8740\n",
      "Epoch [1/5], loss:1.8732\n",
      "Epoch [1/5], loss:1.8725\n",
      "Epoch [1/5], loss:1.8717\n",
      "Epoch [1/5], loss:1.8710\n",
      "Epoch [1/5], loss:1.8703\n",
      "Epoch [1/5], loss:1.8695\n",
      "Epoch [1/5], loss:1.8687\n",
      "Epoch [1/5], loss:1.8680\n",
      "Epoch [1/5], loss:1.8672\n",
      "Epoch [1/5], loss:1.8665\n",
      "Epoch [1/5], loss:1.8658\n",
      "Epoch [1/5], loss:1.8650\n",
      "Epoch [1/5], loss:1.8643\n",
      "Epoch [1/5], loss:1.8636\n",
      "Epoch [1/5], loss:1.8628\n",
      "Epoch [1/5], loss:1.8620\n",
      "Epoch [1/5], loss:1.8613\n",
      "Epoch [1/5], loss:1.8606\n",
      "Epoch [1/5], loss:1.8599\n",
      "Epoch [1/5], loss:1.8592\n",
      "Epoch [1/5], loss:1.8584\n",
      "Epoch [1/5], loss:1.8577\n",
      "Epoch [1/5], loss:1.8569\n",
      "Epoch [1/5], loss:1.8561\n",
      "Epoch [1/5], loss:1.8554\n",
      "Epoch [1/5], loss:1.8547\n",
      "Epoch [1/5], loss:1.8539\n",
      "Epoch [2/5], loss:1.8531\n",
      "Epoch [2/5], loss:1.8523\n",
      "Epoch [2/5], loss:1.8514\n",
      "Epoch [2/5], loss:1.8507\n",
      "Epoch [2/5], loss:1.8500\n",
      "Epoch [2/5], loss:1.8492\n",
      "Epoch [2/5], loss:1.8485\n",
      "Epoch [2/5], loss:1.8477\n",
      "Epoch [2/5], loss:1.8470\n",
      "Epoch [2/5], loss:1.8463\n",
      "Epoch [2/5], loss:1.8456\n",
      "Epoch [2/5], loss:1.8449\n",
      "Epoch [2/5], loss:1.8441\n",
      "Epoch [2/5], loss:1.8434\n",
      "Epoch [2/5], loss:1.8426\n",
      "Epoch [2/5], loss:1.8419\n",
      "Epoch [2/5], loss:1.8412\n",
      "Epoch [2/5], loss:1.8405\n",
      "Epoch [2/5], loss:1.8399\n",
      "Epoch [2/5], loss:1.8392\n",
      "Epoch [2/5], loss:1.8384\n",
      "Epoch [2/5], loss:1.8376\n",
      "Epoch [2/5], loss:1.8369\n",
      "Epoch [2/5], loss:1.8362\n",
      "Epoch [2/5], loss:1.8354\n",
      "Epoch [2/5], loss:1.8347\n",
      "Epoch [2/5], loss:1.8341\n",
      "Epoch [2/5], loss:1.8333\n",
      "Epoch [2/5], loss:1.8326\n",
      "Epoch [2/5], loss:1.8319\n",
      "Epoch [2/5], loss:1.8312\n",
      "Epoch [2/5], loss:1.8305\n",
      "Epoch [2/5], loss:1.8299\n",
      "Epoch [2/5], loss:1.8291\n",
      "Epoch [2/5], loss:1.8283\n",
      "Epoch [2/5], loss:1.8276\n",
      "Epoch [2/5], loss:1.8269\n",
      "Epoch [2/5], loss:1.8262\n",
      "Epoch [2/5], loss:1.8254\n",
      "Epoch [2/5], loss:1.8247\n",
      "Epoch [2/5], loss:1.8240\n",
      "Epoch [2/5], loss:1.8233\n",
      "Epoch [2/5], loss:1.8226\n",
      "Epoch [2/5], loss:1.8219\n",
      "Epoch [2/5], loss:1.8213\n",
      "Epoch [2/5], loss:1.8206\n",
      "Epoch [2/5], loss:1.8199\n",
      "Epoch [2/5], loss:1.8191\n",
      "Epoch [2/5], loss:1.8184\n",
      "Epoch [2/5], loss:1.8176\n",
      "Epoch [2/5], loss:1.8169\n",
      "Epoch [2/5], loss:1.8162\n",
      "Epoch [2/5], loss:1.8155\n",
      "Epoch [2/5], loss:1.8148\n",
      "Epoch [2/5], loss:1.8141\n",
      "Epoch [2/5], loss:1.8133\n",
      "Epoch [2/5], loss:1.8126\n",
      "Epoch [2/5], loss:1.8119\n",
      "Epoch [2/5], loss:1.8112\n",
      "Epoch [2/5], loss:1.8105\n",
      "Epoch [2/5], loss:1.8098\n",
      "Epoch [2/5], loss:1.8092\n",
      "Epoch [2/5], loss:1.8085\n",
      "Epoch [2/5], loss:1.8078\n",
      "Epoch [2/5], loss:1.8071\n",
      "Epoch [2/5], loss:1.8065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_fn, opt, train_dl, valid_dl, metric)\u001b[0m\n\u001b[0;32m      5\u001b[0m loss, _,_ \u001b[38;5;241m=\u001b[39m loss_batch(model, loss_fn, xb, yb ,opt)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluation \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m val_loss , total, val_metric \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# progress\u001b[39;00m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loss_fun, valid_dl, metric)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, loss_fun,valid_dl, metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m# pass eacg batch through model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m         res \u001b[38;5;241m=\u001b[39m [loss_batch(model, loss_fun,xb,yb, metric \u001b[38;5;241m=\u001b[39m metric)\n\u001b[0;32m      5\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m xb,yb \u001b[38;5;129;01min\u001b[39;00m valid_dl]\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# separate \u001b[39;00m\n\u001b[0;32m      7\u001b[0m         losses, nums,metrices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mres)\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, loss_fun,valid_dl, metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m# pass eacg batch through model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m         res \u001b[38;5;241m=\u001b[39m [loss_batch(model, loss_fun,xb,yb, metric \u001b[38;5;241m=\u001b[39m metric)\n\u001b[0;32m      5\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m xb,yb \u001b[38;5;129;01min\u001b[39;00m valid_dl]\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# separate \u001b[39;00m\n\u001b[0;32m      7\u001b[0m         losses, nums,metrices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mres)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(pic\u001b[38;5;241m.\u001b[39mgetbands()))\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(5,model,F.cross_entropy,opt, train_loader, val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1991b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(root = 'data/', train = False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc902187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,lbl = test_dataset[0]\n",
    "plt.imshow(img[0],cmap='gray')\n",
    "print('Shape:',img.shape)\n",
    "print('Label:',lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fdee45ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24723cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim = 1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee129d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9 Predicted: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, lbl = test_dataset[193]\n",
    "plt.imshow(img[0],cmap='gray')\n",
    "print('label:',lbl,'Predicted:',predict_img(img,model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
