{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12653fc",
   "metadata": {},
   "source": [
    "## Explorinig the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75f35b",
   "metadata": {},
   "source": [
    "**We will download the images and then trainthe model by using CNNs and ResNets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5ae637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import tarfile\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b923b8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: .\\wt103_ids.tgz\n"
     ]
    }
   ],
   "source": [
    "# Download\n",
    "data_url = \"http://files.fast.ai/data/wt103_ids.tgz\"\n",
    "download_url(data_url,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69dd3581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'archive (3)\\cifar10\\cifar10'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74591a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of test examples for airplane: 5000\n",
      "['0001.png', '0002.png', '0003.png', '0004.png', '0005.png']\n"
     ]
    }
   ],
   "source": [
    "airplane_files = os.listdir('archive (3)\\\\cifar10\\\\cifar10\\\\train\\\\airplane')\n",
    "print('No of test examples for airplane:', len(airplane_files))\n",
    "print(airplane_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a84eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14ac8e",
   "metadata": {},
   "source": [
    "Look at the sample element from the training dataset . Each element is given as a tuple, containing image tensor and a label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f839bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7922, 0.7922, 0.8000,  ..., 0.8118, 0.8039, 0.7961],\n",
       "         [0.8078, 0.8078, 0.8118,  ..., 0.8235, 0.8157, 0.8078],\n",
       "         [0.8235, 0.8275, 0.8314,  ..., 0.8392, 0.8314, 0.8235],\n",
       "         ...,\n",
       "         [0.8549, 0.8235, 0.7608,  ..., 0.9529, 0.9569, 0.9529],\n",
       "         [0.8588, 0.8510, 0.8471,  ..., 0.9451, 0.9451, 0.9451],\n",
       "         [0.8510, 0.8471, 0.8510,  ..., 0.9373, 0.9373, 0.9412]],\n",
       "\n",
       "        [[0.8000, 0.8000, 0.8078,  ..., 0.8157, 0.8078, 0.8000],\n",
       "         [0.8157, 0.8157, 0.8196,  ..., 0.8275, 0.8196, 0.8118],\n",
       "         [0.8314, 0.8353, 0.8392,  ..., 0.8392, 0.8353, 0.8275],\n",
       "         ...,\n",
       "         [0.8510, 0.8196, 0.7608,  ..., 0.9490, 0.9490, 0.9529],\n",
       "         [0.8549, 0.8471, 0.8471,  ..., 0.9412, 0.9412, 0.9412],\n",
       "         [0.8471, 0.8431, 0.8471,  ..., 0.9333, 0.9333, 0.9333]],\n",
       "\n",
       "        [[0.7804, 0.7804, 0.7882,  ..., 0.7843, 0.7804, 0.7765],\n",
       "         [0.7961, 0.7961, 0.8000,  ..., 0.8039, 0.7961, 0.7882],\n",
       "         [0.8118, 0.8157, 0.8235,  ..., 0.8235, 0.8157, 0.8078],\n",
       "         ...,\n",
       "         [0.8706, 0.8392, 0.7765,  ..., 0.9686, 0.9686, 0.9686],\n",
       "         [0.8745, 0.8667, 0.8627,  ..., 0.9608, 0.9608, 0.9608],\n",
       "         [0.8667, 0.8627, 0.8667,  ..., 0.9529, 0.9529, 0.9529]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, lbl = dataset[0]\n",
    "print(img.shape,lbl)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac63bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Graph\n",
    "import matplotlib.pyplot as plt\n",
    "def show_sample(img, lbl):\n",
    "    print('label: ', dataset.classes[lbl],\"('+str(lbl)+)')\")\n",
    "    plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  airplane ('+str(lbl)+)')\n"
     ]
    }
   ],
   "source": [
    "show_sample(*dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20e3655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_indices(n, val_pct=0.1, seed=99):\n",
    "    # Determinr size of validation\n",
    "    n_val = int(val_pct*n)\n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(n)\n",
    "    # pick first n_val indices for validation set\n",
    "    return idx[n_val:], idx[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bb3103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000\n",
      "Sample validation indices: [33553  9427   199 12447 39489 42724 10822 49498  4144 36958]\n"
     ]
    }
   ],
   "source": [
    "val_pct = 0.2\n",
    "rand_seed = 42\n",
    "train_indices, val_indices = split_indices(len(dataset), val_pct, rand_seed)\n",
    "print(len(train_indices),len(val_indices))\n",
    "print('Sample validation indices:', val_indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "917ffcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader \n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b69b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trianing sampler and data loader\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset, batch_size, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3428677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation sampler and Dataloader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_dl = DataLoader(dataset, batch_size, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1cfb58f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3087617693.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [17]\u001b[1;36m\u001b[0m\n\u001b[1;33m    def show_batch\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afbd20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89607ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3,8, kernel_size=3, stride=1, padding=1),\n",
    "                nn.MaxPool2d(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4067553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape torch.Size([100, 3, 32, 32])\n",
      "out.shape torch.Size([100, 8, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for img, lbl in train_dl:\n",
    "    print('images.shape', img.shape)\n",
    "    out = simple_model(img)\n",
    "    print('out.shape', out.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e41ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Conv2d(3,16,kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),# batch_size = 16 x 16 x 16\n",
    "    \n",
    "        nn.Conv2d(16,16,kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "    \n",
    "    \n",
    "        nn.Conv2d(16,16,kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "    \n",
    "    \n",
    "        nn.Conv2d(16,16,kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "    \n",
    "    \n",
    "        nn.Conv2d(16,16,kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "    \n",
    "       \n",
    "        nn.Flatten(),\n",
    "        nn.Linear(16,10))\n",
    "         \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "257f9977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 3, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[3].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c1a66",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcaa1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model ,loss_func, xb,yb, opt=None, metric = None):\n",
    "    # Generate predictions\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #reset\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        # compute metric\n",
    "        metric_result = metric(preds, yb)\n",
    "        \n",
    "    return loss.item(), len(xb), metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e20048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn , train_dl,valid_dl,\n",
    "    opt_fn = None, lr = None, metric = None):\n",
    "    \n",
    "    # optimizer \n",
    "    if opt_fn is None:\n",
    "        opt_fn = torch.optim.SGD\n",
    "        opt = opt_fn(model.parameters(), lr = lr)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            train_loss,_,_ = loss_batch(model,loss_fn, xb,yb,opt)\n",
    "            \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        res = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = res\n",
    "        \n",
    "        # record loss\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_metrics.append(val_loss)\n",
    "        \n",
    "        # print progress\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], train_loss: {:4f}, val_loss: {:4f}'.format(epoch+1, epochs, train_loss, val_loss))\n",
    "            \n",
    "        else:\n",
    "            print('Epoch [{}/{}], train_loss: {:4f}, val_loss: {:4f}, val_{}: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss,\n",
    "                                                                                            metric.__name, val_metric))\n",
    "            \n",
    "    return train_losses, val_losses, val_metrics\n",
    "            \n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "235d59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, lbl):\n",
    "    _, preds = torch.max(outputs, dim =1)\n",
    "    return torch.sum(preds == lbl).items()/ len(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c13f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bfc85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(metruc_values):\n",
    "    plt.plot(metric_values,'-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs No. of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb09846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot by yourself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
